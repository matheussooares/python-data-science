{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de conjuntos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest e Gradient Boosted Decision Trees são os tópicos abordados nessa aula. Assim, Random Forest, do inglês Floresta aleatória e o  modelo de Boosted é um conjunto de modelos é criado de modo que cada modelo tente melhorar a performace de um modelo anterior.\n",
    "\n",
    "São modelos que se utilizam de arvore de modelo. As arvores de dedições são interpretáveis, assim propensas a overfit. Os modelos de gradient Bossting evitam o overfit natural das Árvore de Decisão, usando o conceito de wear learners, do inglês aprendizado lento ou fraco. A ideia é contruir um série de modelos diferente e modelos mais fracos que o modelo original mas tentando utilizar a vombinação de pequenos modelos ou modelos simples para construir modelos complexos. \n",
    "\n",
    "**Random Forest**: Utilização de árvores em paralelo.\n",
    "\n",
    "**Gradient Boosted Decision Trees**: Utilização de árvore em série"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo se utiliza do conceito de paralelizadação de decisão, funcinando dado o conjunto de dado e dividindo-o em conjuntos de linha e colunas, podendo várias combinaçõe do conjunto de dados. Com isso, será construido árvores de decisão comuns sobre esses pedaços do conjunto de dados.\n",
    "\n",
    "Um exemplo de divisão seria dois conjuntos de 100 linhas (elementos) e 5 colunas. Ou pode ser dividido dois conjuntos de 50 linhas e 10 colunas, sendo possível efetuar várias divisões aleatoriamente. \n",
    "\n",
    "O nome vem de floresta, já que é um conjunto de arvores e aleátoria porque a divisão do conjunto de dados é aleatório.\n",
    "\n",
    "Árvores de decisão cumuns atuavam sobre o conjunto colpmeto dos daods, apresentando regras que dividiam perfeitamente os mesmos, podendo causar overfit. Uma vantagem desse método é que cada árvore é responsável por um pedaço do conjunto de dados. Assim, se existir overfit será local, o que não influi no cenário total representado pelos dados. \n",
    "\n",
    "No final é utilizado uma técnica de votação na qual a maioria do nosso conjunto de dados ou a maioria das classificações, será a escolhida.\n",
    "\n",
    "**Problema de classificação**: Escolhe-se a classe eleita pela maioria das arvores.\n",
    "\n",
    "**Problema de Regressão**: escolhe-se a média dos resultados obtidos pelas árvores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Teorema**: Se mais da metade das arvores decidirem por uma resposta, esta resposta é correta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a8a96d336aa61fc63de487414a9c7e1efb8cdc0e8ce2ffe1d9b942901e306453"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
